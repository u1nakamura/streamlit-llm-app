from dotenv import load_dotenv

load_dotenv()

import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from dotenv import load_dotenv
import os

# .envã®èª­ã¿è¾¼ã¿
load_dotenv()

# LangChainã®LLMåˆæœŸåŒ–ï¼ˆOpenAI APIã‚­ãƒ¼ã¯.envã«è¨­å®šæ¸ˆã¿ã§ã‚ã‚‹å‰æï¼‰
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.7)

# å°‚é–€å®¶ã”ã¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
expert_messages = {
    "åŒ»ç™‚ã®å°‚é–€å®¶": "ã‚ãªãŸã¯æ‚£è€…ã®ç—‡çŠ¶ã‚„å¥åº·ç›¸è«‡ã«å„ªã—ãä¸å¯§ã«ç­”ãˆã‚‹åŒ»ç™‚ã®å°‚é–€å®¶ã§ã™ã€‚",
    "æ³•å¾‹ã®å°‚é–€å®¶": "ã‚ãªãŸã¯ä¸€èˆ¬å¸‚æ°‘ã®æ³•å¾‹ç›¸è«‡ã«å¯¾ã—ã¦çš„ç¢ºã‹ã¤å¹³æ˜“ã«èª¬æ˜ã™ã‚‹å¼è­·å£«ã§ã™ã€‚",
}

# LLMã¸ã®å•ã„åˆã‚ã›é–¢æ•°
def ask_llm(user_input: str, expert_type: str) -> str:
    system_prompt = expert_messages.get(expert_type, "ã‚ãªãŸã¯å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚")
    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_prompt),
        HumanMessagePromptTemplate.from_template("{question}")
    ])
    chain = LLMChain(llm=llm, prompt=prompt)
    response = chain.run({"question": user_input})
    return response

# Streamlit UIæ§‹æˆ
st.title("ğŸ§  å°‚é–€å®¶ã«è³ªå•ã§ãã‚‹LLMã‚¢ãƒ—ãƒª")
st.write("ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒ ã«è³ªå•ã‚’å…¥åŠ›ã—ã€å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠã—ã¦é€ä¿¡ã—ã¦ãã ã•ã„ã€‚")

expert_type = st.radio("å°‚é–€å®¶ã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„", list(expert_messages.keys()))
user_input = st.text_area("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

if st.button("é€ä¿¡"):
    if user_input.strip():
        with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
            answer = ask_llm(user_input, expert_type)
        st.success("å›ç­”:")
        st.write(answer)
    else:
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")